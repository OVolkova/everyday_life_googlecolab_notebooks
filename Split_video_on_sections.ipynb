{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OVolkova/everyday_life_googlecolab_notebooks/blob/main/Split_video_on_sections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api\n",
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "88Re9mDD4GBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get sections of youtube video.\n",
        "You need to have video id and openai api key for that"
      ],
      "metadata": {
        "id": "pgs4cedjZPDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "8l_VVPxa4Jcl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set OpenAI api key"
      ],
      "metadata": {
        "id": "OWHLNzyrE9KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = <put here your key>\n",
        "os.environ['OPENAI_API_KEY'] = openai.api_key"
      ],
      "metadata": {
        "id": "2ZVcpd0a6am8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set pydantic classes for tools"
      ],
      "metadata": {
        "id": "7jwXwuykFGRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Section(BaseModel):\n",
        "    \"\"\"Section of video.\"\"\"\n",
        "    name: str = Field(description=\"Provide a short maximum 5 words name of the section\")\n",
        "    summary: str = Field(description=\"Provide a concise  1 sentence summary of the content.\")\n",
        "    start_time: str = Field(description=\"Provide the start time of section. Use float format with '.' as separator\")\n",
        "    end_time: str = Field(description=\"Provide the end time of section\")\n",
        "\n",
        "class Info(BaseModel):\n",
        "    \"\"\"information to extract\"\"\"\n",
        "    sections: List[Section]"
      ],
      "metadata": {
        "id": "uYoaLrr-7H9_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set prompt"
      ],
      "metadata": {
        "id": "ELISTQNYFLbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    It is a video transcript from the middle of lecture.\n",
        "    Think carefully, and then extract list sections of the video as instructed.\n",
        "    Rules to follow:\n",
        "    1. There is no introdution in the lecture.\n",
        "    2. Never name section Introduction\n",
        "    3. Each Section should be 3-10 minutes.\n",
        "    4. Start time of next section must be equal to the end time of previous.\n",
        "    \"\"\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "TvaCD_xi7vEb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set chain with the tool"
      ],
      "metadata": {
        "id": "eTJJTVGsFOlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0, model = 'gpt-4-1106-preview')\n",
        "\n",
        "overview_tagging_function = [\n",
        "    convert_pydantic_to_openai_function(Info)\n",
        "]\n",
        "tagging_model = model.bind(\n",
        "    functions=overview_tagging_function,\n",
        "    function_call={\"name\":\"Info\"}\n",
        ")\n",
        "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "BVLdevsd7j_S"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper functions to do the logic"
      ],
      "metadata": {
        "id": "vAHolzK8GWOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_youtube_subtitles(video_id: str)-> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Load subtitles and prepare\n",
        "  \"\"\"\n",
        "  result = pd.DataFrame(YouTubeTranscriptApi.get_transcript(video_id))\n",
        "  result['start'] = (result['start']/60).round(1)\n",
        "  result['text'] = result.groupby('start')['text'].transform(lambda x: ' '.join(x))\n",
        "  result = result[['text', 'start']].drop_duplicates()\n",
        "  result.reset_index(drop=True, inplace=True)\n",
        "  return result\n",
        "\n",
        "\n",
        "def format_model_output(result: dict) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Format model output json into dataframe and enrich with information\n",
        "  \"\"\"\n",
        "  final = pd.DataFrame(result)\n",
        "  final['end_time'] = final['end_time'].str.replace(':', '.').astype(float)\n",
        "  final['start_time'] = final['start_time'].str.replace(':', '.').astype(float)\n",
        "  final['duration'] = final['end_time'] - final['start_time']\n",
        "  final['progress %'] = (final['end_time'] / final['end_time'].max() * 100).round(0).astype(int)\n",
        "\n",
        "  final['summary'] = final['summary'].apply(split_on_lines)\n",
        "  return final\n",
        "\n",
        "\n",
        "def split_on_lines(text: str, total_in_line: int=50) -> str:\n",
        "  \"\"\"\n",
        "  Split long summary on lines for pretty print\n",
        "  \"\"\"\n",
        "\n",
        "  l = text.replace('\\n', '').split(' ')\n",
        "  t = 0\n",
        "  result = ''\n",
        "  for w in l:\n",
        "    if t + len(w) >=total_in_line:\n",
        "      t = 0\n",
        "      result +='\\\\n'\n",
        "    result += w + ' '\n",
        "    t+=len(w)\n",
        "  return result\n",
        "\n",
        "\n",
        "def get_video_plan_from_subtitles(subtitles: pd.DataFrame, tagging_chain, chunk_size: int=13000) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Send subtitels to llm-chain to extract sections.\n",
        "  Subtitles are long, so it's split on chunks\n",
        "  \"\"\"\n",
        "  last_chunk = False\n",
        "  i = 0\n",
        "  sections = []\n",
        "\n",
        "  s = subtitles.to_csv(index=False)\n",
        "\n",
        "  while True:\n",
        "    print('processing chunk ', i)\n",
        "    i+=1\n",
        "    r =  tagging_chain.invoke({\"input\": s[:chunk_size]})\n",
        "\n",
        "    if last_chunk:\n",
        "      sections.extend(r['sections'])\n",
        "      break\n",
        "    sections.extend(r['sections'][:-1])\n",
        "    s = subtitles[subtitles[subtitles['start']==float(r['sections'][-1]['start_time'].replace(':', '.'))].index[0]:].to_csv(index=False)\n",
        "    if len(s) < chunk_size:\n",
        "      last_chunk = True\n",
        "\n",
        "  return sections\n",
        "\n",
        "\n",
        "def pretty_print(df: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Pretty_print pandas as html\n",
        "  \"\"\"\n",
        "  return display(HTML(df.to_html().replace(\"\\\\n\",\"<br>\")))\n",
        "\n",
        "\n",
        "def pipeline(video_id: str, tagging_chain, chunk_size: int=13000) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Pipeline to split extract sections of youtube video\n",
        "  \"\"\"\n",
        "  subtitles = get_youtube_subtitles(video_id)\n",
        "  sections = get_video_plan_from_subtitles(subtitles, tagging_chain, chunk_size=chunk_size)\n",
        "  return format_model_output(sections)"
      ],
      "metadata": {
        "id": "2wkyJ-n_H8AT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the video"
      ],
      "metadata": {
        "id": "xVrkQIKMK_z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sections = pipeline(video_id = \"plIJYzVKfdI\", tagging_chain=tagging_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vKQyNdbIHDX",
        "outputId": "15ab57a9-e824-43bc-fb00-d897b9096636"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing chunk  0\n",
            "processing chunk  1\n",
            "processing chunk  2\n",
            "processing chunk  3\n",
            "processing chunk  4\n",
            "processing chunk  5\n",
            "processing chunk  6\n",
            "processing chunk  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(sections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kOhD7t-qNQlq",
        "outputId": "76830135-47c2-491a-fa35-5c8a8541eb2e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>summary</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>duration</th>\n",
              "      <th>progress %</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Statistical Learning Basics</td>\n",
              "      <td>The lecture begins with an introduction to the basic <br>elements of statistical learning, including data, <br>models, error metrics, and estimation algorithms.</td>\n",
              "      <td>0.7</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data and Supervised Learning</td>\n",
              "      <td>The lecturer discusses data in the context of supervised <br>learning, focusing on the input-output relationship and <br>the assumption of a noiseless label generated by an <br>unknown function.</td>\n",
              "      <td>2.9</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Model and Complexity</td>\n",
              "      <td>The concept of a model as a class of parameterized <br>functions is introduced, along with the notion of <br>complexity measures to organize hypotheses.</td>\n",
              "      <td>5.5</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Error Metrics Overview</td>\n",
              "      <td>The lecturer discusses error metrics, focusing on the <br>difference between population loss (test error) and <br>empirical loss (training error), and their <br>relationship.</td>\n",
              "      <td>10.7</td>\n",
              "      <td>12.9</td>\n",
              "      <td>2.2</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Understanding Loss Functions</td>\n",
              "      <td>The lecturer explains the concept of loss functions, <br>their deterministic and random nature, and the <br>importance of understanding the fluctuations between <br>the empirical and population loss.</td>\n",
              "      <td>13.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.9</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Uniform Bounds and Variance</td>\n",
              "      <td>The lecturer addresses the need for uniform bounds to <br>control the fluctuation between empirical and <br>population loss across all hypotheses, not just a fixed <br>one.</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.2</td>\n",
              "      <td>2.2</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Algorithmic Principles</td>\n",
              "      <td>The lecturer introduces the principles of learning <br>algorithms in supervised learning, focusing on <br>empirical risk minimization and the importance of <br>hypothesis complexity.</td>\n",
              "      <td>18.3</td>\n",
              "      <td>21.5</td>\n",
              "      <td>3.2</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Constraint Optimization and Regularization</td>\n",
              "      <td>The lecture discusses the transformation of constraint <br>optimization problems to unconstrained ones using <br>Lagrange multipliers and introduces regularization as a <br>method to add complexity into the loss function.</td>\n",
              "      <td>21.6</td>\n",
              "      <td>22.5</td>\n",
              "      <td>0.9</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Interpolation and Overparameterization</td>\n",
              "      <td>The concept of interpolation in machine learning is <br>explained, particularly in the context of neural <br>networks with many parameters, and how it relates to <br>fitting all data points.</td>\n",
              "      <td>22.6</td>\n",
              "      <td>24.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Analyzing ERM Algorithm Properties</td>\n",
              "      <td>The lecture begins to analyze the properties of the <br>Empirical Risk Minimization (ERM) algorithm by <br>considering an arbitrary hypothesis class and comparing <br>hypotheses.</td>\n",
              "      <td>24.3</td>\n",
              "      <td>26.9</td>\n",
              "      <td>2.6</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Decomposing Generalization Error</td>\n",
              "      <td>The lecture explains how to decompose the generalization <br>error into different components, including statistical <br>error, approximation error, and optimization error.</td>\n",
              "      <td>27.0</td>\n",
              "      <td>32.7</td>\n",
              "      <td>5.7</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Understanding Loss Functions</td>\n",
              "      <td>The lecturer uses a diagram to explain the relationship <br>between the population loss function and the empirical <br>loss function, and how the algorithm focuses on a subset of <br>functions within a hypothesis class.</td>\n",
              "      <td>32.8</td>\n",
              "      <td>34.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ERM Algorithm and Error Decomposition</td>\n",
              "      <td>The lecturer discusses the Empirical Risk Minimization <br>(ERM) algorithm and how it aims to find the function with <br>the lowest empirical loss within a hypothesis class, and <br>introduces the concept of error decomposition.</td>\n",
              "      <td>34.1</td>\n",
              "      <td>36.9</td>\n",
              "      <td>2.8</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Challenges of High-Dimensional Learning</td>\n",
              "      <td>The lecturer explains the difficulties in controlling <br>approximation and statistical errors in <br>high-dimensional spaces and the ongoing challenge of <br>developing a comprehensive theory for neural networks.</td>\n",
              "      <td>37.0</td>\n",
              "      <td>39.4</td>\n",
              "      <td>2.4</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Curse of Dimensionality</td>\n",
              "      <td>The lecture discusses the curse of dimensionality and its <br>impact on machine learning, particularly as the input <br>size of a problem increases.</td>\n",
              "      <td>39.5</td>\n",
              "      <td>42.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Proximity in Learning</td>\n",
              "      <td>The concept of proximity in learning is explained, <br>highlighting how knowing the value of a function at <br>certain points allows us to infer its value elsewhere.</td>\n",
              "      <td>42.7</td>\n",
              "      <td>46.9</td>\n",
              "      <td>4.2</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Learning Lipschitz Functions</td>\n",
              "      <td>The lecturer discusses a natural algorithm for learning <br>Lipschitz functions, including the concept of Lipschitz <br>constant as a measure of complexity.</td>\n",
              "      <td>46.9</td>\n",
              "      <td>49.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Defining Lipschitz Functions</td>\n",
              "      <td>The lecturer explains the definition of Lipschitz <br>functions and provides examples of functions that are and <br>are not Lipschitz.</td>\n",
              "      <td>50.0</td>\n",
              "      <td>54.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Estimating with Lipschitz Functions</td>\n",
              "      <td>The lecturer introduces an estimator for Lipschitz <br>functions and poses a question about the error of <br>approximation.</td>\n",
              "      <td>54.6</td>\n",
              "      <td>56.9</td>\n",
              "      <td>2.3</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Lower Bounds and Complexity</td>\n",
              "      <td>The lecturer hints at how to think about lower bounds and <br>complexity in the context of learning Lipschitz <br>functions.</td>\n",
              "      <td>57.0</td>\n",
              "      <td>62.3</td>\n",
              "      <td>5.3</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Curse of Dimensionality</td>\n",
              "      <td>The speaker discusses the curse of dimensionality in <br>statistical learning and approximation, particularly <br>in neural networks with a single hidden layer, and how it <br>affects the number of observations needed and the <br>approximation error.</td>\n",
              "      <td>62.4</td>\n",
              "      <td>63.8</td>\n",
              "      <td>1.4</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Neural Network Complexity</td>\n",
              "      <td>The lecture covers the complexity of neural networks, <br>focusing on the width and other measures like weight <br>decay, and introduces the concept of universal <br>approximation theorem.</td>\n",
              "      <td>63.8</td>\n",
              "      <td>65.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Approximation Rates and Dimension</td>\n",
              "      <td>The speaker explains how the approximation rates of <br>neural networks are affected by the dimension of the data, <br>particularly when functions have a certain number of <br>derivatives.</td>\n",
              "      <td>65.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Baron Class and Regularity</td>\n",
              "      <td>The concept of the Baron class is introduced as a measure of <br>regularity tailored to shallow neural networks, which <br>does not suffer from the curse of dimensionality.</td>\n",
              "      <td>67.0</td>\n",
              "      <td>69.4</td>\n",
              "      <td>2.4</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Optimization Challenges</td>\n",
              "      <td>The challenges of optimization in high dimensions are <br>discussed, including the difficulty of finding global <br>minima and the exponential growth of points needed to <br>discretize the input domain.</td>\n",
              "      <td>69.4</td>\n",
              "      <td>71.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Overcoming Optimization Curse</td>\n",
              "      <td>The speaker talks about how certain landscapes, unlike <br>the worst-case scenarios, allow for efficient <br>optimization without the curse of dimensionality, and <br>how gradient descent can be effective for these classes of <br>functions.</td>\n",
              "      <td>71.6</td>\n",
              "      <td>74.2</td>\n",
              "      <td>2.6</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Learning in High Dimensions</td>\n",
              "      <td>The lecturer discusses the challenges of learning <br>functions in high dimensions and the trade-off between <br>statistical error and approximation error.</td>\n",
              "      <td>74.2</td>\n",
              "      <td>75.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Geometry of Learning Problems</td>\n",
              "      <td>The lecturer introduces the concept of leveraging the <br>physical structure of data to create more tailored spaces <br>of functions for learning.</td>\n",
              "      <td>75.5</td>\n",
              "      <td>79.6</td>\n",
              "      <td>4.1</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}